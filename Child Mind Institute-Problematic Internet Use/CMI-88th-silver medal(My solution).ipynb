{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My solution\n",
    "\n",
    "Train three models and combine the predictions of the three models as the final prediction:\n",
    "\n",
    "1. Model 1\n",
    "\n",
    "    + Extracting features from time series\n",
    "\n",
    "        + skew, kurtosis and statistical information (max, min, std, mean, ...)\n",
    "\n",
    "        + Calculation of statistical information by time period \n",
    "    \n",
    "    + Encode the features from time series\n",
    "\n",
    "    + Process outliers in train.csv and test.csv\n",
    "\n",
    "    + use KNN and MICE fill the nan values, except 'sii'(No filling on the 'sii')\n",
    "\n",
    "    + train model[ensemble model(LGBM, XGB, HistGradientBoosting, CatBoost, TabNet)] by `RepeatedKFold`(Training results are more robust)\n",
    "\n",
    "2. Model 2 \n",
    "\n",
    "    + Extracting features from time series\n",
    "\n",
    "        + skew, kurtosis and statistical information (max, min, std, mean, ...)\n",
    "\n",
    "    + use `SimpleImputer` fill the nan values\n",
    "\n",
    "    + train model[ensemble model(LGBM, XGB, GradientBoosting, CatBoost, RandomForest)] by `RepeatedKFold`(Training results are more robust)\n",
    "\n",
    "3. Model 3\n",
    "\n",
    "    + semi supervised: Trained first with non-missing data, labelled against pseudo-labels, and later trained against pseudo-labels [ensemble model(LGBM, XGB, CatBoost)] by `RepeatedKFold`(Training results are more robust)\n",
    "\n",
    "\n",
    "Using `RepeatedKFold` can get more stable training results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T03:14:15.038055Z",
     "iopub.status.busy": "2024-12-09T03:14:15.037673Z",
     "iopub.status.idle": "2024-12-09T03:14:56.248456Z",
     "shell.execute_reply": "2024-12-09T03:14:56.24748Z",
     "shell.execute_reply.started": "2024-12-09T03:14:15.038025Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip -q install /kaggle/input/pytorchtabnet/pytorch_tabnet-4.1.0-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T04:36:03.954252Z",
     "iopub.status.busy": "2024-12-09T04:36:03.953455Z",
     "iopub.status.idle": "2024-12-09T04:36:03.96522Z",
     "shell.execute_reply": "2024-12-09T04:36:03.964123Z",
     "shell.execute_reply.started": "2024-12-09T04:36:03.954215Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm\n",
    "import polars as pl\n",
    "from scipy.stats import skew, kurtosis\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.base import clone,BaseEstimator, RegressorMixin\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.semi_supervised import LabelPropagation,LabelSpreading\n",
    "from sklearn.model_selection import train_test_split,StratifiedKFold,RepeatedKFold\n",
    "from sklearn.preprocessing import LabelEncoder,StandardScaler,RobustScaler,MinMaxScaler\n",
    "from sklearn.experimental import enable_iterative_imputer  # noqa\n",
    "from sklearn.impute import KNNImputer, IterativeImputer, SimpleImputer\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor,VotingRegressor,AdaBoostRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "\n",
    "from pytorch_tabnet.callbacks import Callback\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from colorama import Fore, Style\n",
    "from IPython.display import clear_output\n",
    "import warnings\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.options.display.max_columns = None\n",
    "import optuna\n",
    "\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Lambda\n",
    "from tensorflow.keras.regularizers import l1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T04:56:39.790545Z",
     "iopub.status.busy": "2024-12-09T04:56:39.78989Z",
     "iopub.status.idle": "2024-12-09T04:56:39.801415Z",
     "shell.execute_reply": "2024-12-09T04:56:39.80053Z",
     "shell.execute_reply.started": "2024-12-09T04:56:39.790511Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "seed_everything(2024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process time series file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract daily feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T03:16:09.959488Z",
     "iopub.status.busy": "2024-12-09T03:16:09.959101Z",
     "iopub.status.idle": "2024-12-09T03:16:09.975052Z",
     "shell.execute_reply": "2024-12-09T03:16:09.974166Z",
     "shell.execute_reply.started": "2024-12-09T03:16:09.95946Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def process_file(filename, dirname):\n",
    "    df = pd.read_parquet(os.path.join(dirname, filename, 'part-0.parquet'))\n",
    "    df.drop('step', axis=1, inplace=True)\n",
    "    df['battery_voltage_percent'] = df['battery_voltage'] / 5000\n",
    "    df['time_of_day'] = df['time_of_day'] / 1000000000\n",
    "    df['hour'] = pd.to_datetime(df['time_of_day'], unit='s').dt.hour\n",
    "    \n",
    "    df['total_accel'] = np.sqrt(df['X']**2 + df['Y']**2 + df['Z']**2)\n",
    "    df['abs_anglez'] = np.abs(df['anglez'])\n",
    "    \n",
    "    df['diff_total_accel'] = df['total_accel'].diff().abs()\n",
    "    df['diff_light'] = df['light'].diff().abs()\n",
    "    df['light_spike'] = df['diff_light'] > df['diff_light'].mean()\n",
    "    df['acc_spike'] = df['diff_total_accel'] > df['diff_total_accel'].mean()\n",
    "    \n",
    "    select_ts_col = ['X','Y','Z','enmo','anglez','non-wear_flag','light','battery_voltage_percent',\n",
    "     'weekday','quarter','relative_date_PCIAT','hour','total_accel','abs_anglez']\n",
    "    select_ts_col_0 = ['X','Y','Z','enmo','anglez','light','battery_voltage_percent',\n",
    "     'weekday','hour','total_accel','abs_anglez']\n",
    "    \n",
    "    skew_info = skew(df[select_ts_col_0])\n",
    "    kurtosis_info = kurtosis(df[select_ts_col_0])\n",
    "    stats_info = df[select_ts_col].describe().values.reshape(-1)[len(select_ts_col):]\n",
    "    extract_info =  np.concatenate((stats_info, skew_info, kurtosis_info))\n",
    "    \n",
    "    stats_feature = pd.DataFrame(extract_info.reshape(1, -1), columns=[f\"stat_{i}\" for i in range(len(extract_info))])\n",
    "    \n",
    "    awake_agg_functions = {\n",
    "            'enmo': ['mean', 'std', 'skew', ('kurtosis', lambda x: kurtosis(x, nan_policy='omit'))],\n",
    "            'total_accel': ['mean', 'std',  'skew', ('kurtosis', lambda x: kurtosis(x, nan_policy='omit'))],\n",
    "            'abs_anglez': ['mean', 'std', 'max', 'min', 'skew', ('kurtosis', lambda x: kurtosis(x, nan_policy='omit'))],\n",
    "            'light': ['mean', 'std'],\n",
    "            'battery_voltage_percent': ['mean', 'std'],\n",
    "            'acc_spike': ['sum'],\n",
    "        }\n",
    "    \n",
    "    sleep_agg_functions = {\n",
    "            'enmo': ['mean', 'std', 'skew', ('kurtosis', lambda x: kurtosis(x, nan_policy='omit'))],\n",
    "            'total_accel': ['mean', 'std',  'skew', ('kurtosis', lambda x: kurtosis(x, nan_policy='omit'))],\n",
    "            'abs_anglez': ['mean', 'std', 'max', 'min', 'skew', ('kurtosis', lambda x: kurtosis(x, nan_policy='omit'))],\n",
    "            'light': ['mean', 'std','max'],\n",
    "            'light_spike': ['sum'],\n",
    "            'acc_spike': ['sum'],\n",
    "        }\n",
    "    act_df = df[df['hour'].between(7, 22)]\n",
    "    \n",
    "    sleep_df = df[(df['hour'].between(0, 6)) | (df['hour']==23)]\n",
    "    \n",
    "    workday_act = act_df[act_df['weekday'].between(1, 5)]\n",
    "    weekend_act = act_df[act_df['weekday'].between(6, 7)]\n",
    "    workday_sleep = sleep_df[sleep_df['weekday'].between(1, 5)]\n",
    "    weekend_sleep = sleep_df[sleep_df['weekday'].between(6, 7)]\n",
    "    \n",
    "    workday_act_features = workday_act.groupby('relative_date_PCIAT').agg(awake_agg_functions)\n",
    "    weekend_act_features = weekend_act.groupby('relative_date_PCIAT').agg(awake_agg_functions)\n",
    "    workday_sleep_features = workday_sleep.groupby('relative_date_PCIAT').agg(sleep_agg_functions)\n",
    "    weekend_sleep_features = weekend_sleep.groupby('relative_date_PCIAT').agg(sleep_agg_functions)\n",
    "    \n",
    "    \n",
    "    # rename columns\n",
    "    workday_act_features.columns = ['work_daily_' + '_'.join(col) for col in workday_act_features.columns.values]\n",
    "    weekend_act_features.columns = ['wekd_daily_' + '_'.join(col) for col in weekend_act_features.columns.values]\n",
    "    workday_sleep_features.columns = ['work_sleep_' + '_'.join(col) for col in workday_sleep_features.columns.values]\n",
    "    weekend_sleep_features.columns = ['wekd_sleep_' + '_'.join(col) for col in weekend_sleep_features.columns.values]\n",
    "    \n",
    "    \n",
    "    \n",
    "    peaks, _ = find_peaks(df['time_of_day'], height=0)\n",
    "    peak_intervals = np.diff(peaks)\n",
    "    \n",
    "    workday_act_features['peak_interval_mean'] = np.mean(peak_intervals) if len(peak_intervals) > 0 else 0\n",
    "    workday_act_features['peak_interval_std'] = np.std(peak_intervals) if len(peak_intervals) > 0 else 0\n",
    "    \n",
    "    \n",
    "    daily_feature = pd.concat([workday_act_features.mean(), weekend_act_features.mean(),workday_sleep_features.mean(),weekend_sleep_features.mean()]).to_frame().T\n",
    "    \n",
    "    result = stats_feature.join(daily_feature)\n",
    "    \n",
    "    result['id'] = filename.split('=')[1]\n",
    "    return result\n",
    "\n",
    "\n",
    "def load_time_series(dirname) -> pd.DataFrame:\n",
    "    ids = os.listdir(dirname)\n",
    "    \n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        results = list(tqdm(executor.map(lambda fname: process_file(fname, dirname), ids), total=len(ids)))\n",
    "    \n",
    "    df = pd.concat(results, ignore_index=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T03:16:12.259487Z",
     "iopub.status.busy": "2024-12-09T03:16:12.259152Z",
     "iopub.status.idle": "2024-12-09T03:25:08.171512Z",
     "shell.execute_reply": "2024-12-09T03:25:08.170404Z",
     "shell.execute_reply.started": "2024-12-09T03:16:12.259455Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "SEED = 42\n",
    "n_splits = 5\n",
    "n_repeats = 5\n",
    "# Load datasets\n",
    "train = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/train.csv')\n",
    "test = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/test.csv')\n",
    "sample = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/sample_submission.csv')\n",
    "\n",
    "train_ts = load_time_series(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_train.parquet\")\n",
    "test_ts = load_time_series(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_test.parquet\")\n",
    "# fillna as 0\n",
    "train_ts = train_ts.fillna(0)\n",
    "test_ts = test_ts.fillna(0)\n",
    "\n",
    "train_ts.to_csv('train_ts.csv')\n",
    "test_ts.to_csv('test_ts.csv')\n",
    "# copy loaded tiem series\n",
    "train_ts_copy = train_ts.filter(regex='^stat_|^id')\n",
    "test_ts_copy = test_ts.filter(regex='^stat_|^id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T03:25:08.344677Z",
     "iopub.status.busy": "2024-12-09T03:25:08.344338Z",
     "iopub.status.idle": "2024-12-09T03:25:08.351114Z",
     "shell.execute_reply": "2024-12-09T03:25:08.350222Z",
     "shell.execute_reply.started": "2024-12-09T03:25:08.34464Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(f'train.shape:{train.shape}')\n",
    "\n",
    "print(f'train_ts.shape:{train_ts.shape}')\n",
    "\n",
    "print(f'test.shape:{test.shape}')\n",
    "\n",
    "print(f'test_ts.shape:{test_ts.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode daily feature "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T03:25:08.353223Z",
     "iopub.status.busy": "2024-12-09T03:25:08.352937Z",
     "iopub.status.idle": "2024-12-09T03:25:08.602263Z",
     "shell.execute_reply": "2024-12-09T03:25:08.601196Z",
     "shell.execute_reply.started": "2024-12-09T03:25:08.353195Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Feature selection based on variance threshold\n",
    "feature_names = train_ts.drop(['id'], axis=1).columns\n",
    "selector = VarianceThreshold(threshold=0.05)\n",
    "train_ts_filtered = selector.fit_transform(train_ts.drop(['id'], axis=1))\n",
    "train_ts_filtered = pd.DataFrame(train_ts_filtered, columns=feature_names[selector.get_support()])\n",
    "\n",
    "test_ts_filtered = test_ts[train_ts_filtered.columns]\n",
    "\n",
    "# calculate correlation matrix\n",
    "correlation_matrix = train_ts_filtered.corr()\n",
    "correlation_threshold = 0.85  \n",
    "\n",
    "\n",
    "to_drop = set() \n",
    "\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > correlation_threshold:\n",
    "            colname = correlation_matrix.columns[i]\n",
    "            to_drop.add(colname)\n",
    "\n",
    "train_ts_filtered = train_ts_filtered.drop(to_drop, axis=1)\n",
    "test_ts_filtered = test_ts[train_ts_filtered.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T05:04:04.171573Z",
     "iopub.status.busy": "2024-12-09T05:04:04.171243Z",
     "iopub.status.idle": "2024-12-09T05:04:04.241535Z",
     "shell.execute_reply": "2024-12-09T05:04:04.240668Z",
     "shell.execute_reply.started": "2024-12-09T05:04:04.171543Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_ts_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T05:08:46.269827Z",
     "iopub.status.busy": "2024-12-09T05:08:46.269453Z",
     "iopub.status.idle": "2024-12-09T05:08:46.316716Z",
     "shell.execute_reply": "2024-12-09T05:08:46.31578Z",
     "shell.execute_reply.started": "2024-12-09T05:08:46.269796Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "non_stat_columns = [col for col in train_ts_filtered.columns if not col.startswith(\"stat_\")]\n",
    "stat_columns = [col for col in train_ts_filtered.columns if col.startswith(\"stat_\")]\n",
    "train_ts_filtered[stat_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T03:28:51.02182Z",
     "iopub.status.busy": "2024-12-09T03:28:51.021451Z",
     "iopub.status.idle": "2024-12-09T03:28:51.02604Z",
     "shell.execute_reply": "2024-12-09T03:28:51.025109Z",
     "shell.execute_reply.started": "2024-12-09T03:28:51.021788Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2024-12-09T05:10:34.091732Z",
     "iopub.status.busy": "2024-12-09T05:10:34.091386Z",
     "iopub.status.idle": "2024-12-09T05:10:39.948747Z",
     "shell.execute_reply": "2024-12-09T05:10:39.948079Z",
     "shell.execute_reply.started": "2024-12-09T05:10:34.09169Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def build_autoencoder(input_dim, encoding_dim):\n",
    "    input_layer = Input(shape=(input_dim,))\n",
    "    encoded = Dense(encoding_dim, activation='relu')(input_layer)\n",
    "    decoded = Dense(input_dim, activation='sigmoid')(encoded)\n",
    "    \n",
    "    autoencoder = Model(inputs=input_layer, outputs=decoded)\n",
    "    encoder = Model(inputs=input_layer, outputs=encoded)\n",
    "    \n",
    "    autoencoder.compile(optimizer=Adam(), loss='mse')\n",
    "    \n",
    "    return autoencoder, encoder\n",
    "\n",
    "def perform_autoencoder(df_train, df_test=None, encoding_dim=50, epochs=50, batch_size=32):\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    df_train_scaled = scaler.fit_transform(df_train)\n",
    "    \n",
    "    input_dim = df_train_scaled.shape[1]\n",
    "    autoencoder, encoder = build_autoencoder(input_dim, encoding_dim)\n",
    "    \n",
    "    autoencoder.fit(df_train_scaled, df_train_scaled, epochs=epochs, batch_size=batch_size, shuffle=True, verbose=1)\n",
    "    \n",
    "    encoded_train_data = encoder.predict(df_train_scaled)\n",
    "    df_encoded_train = pd.DataFrame(encoded_train_data, columns=[f'Enc_{i+1}' for i in range(encoded_train_data.shape[1])])\n",
    "    \n",
    "    if df_test is not None:\n",
    "        \n",
    "        df_test_scaled = scaler.transform(df_test)\n",
    "        encoded_test_data = encoder.predict(df_test_scaled)\n",
    "        df_encoded_test = pd.DataFrame(encoded_test_data, columns=[f'Enc_{i+1}' for i in range(encoded_test_data.shape[1])])\n",
    "        \n",
    "        return df_encoded_train, df_encoded_test\n",
    "    \n",
    "    return df_encoded_train\n",
    "\n",
    "\n",
    "# encoded_train_ts, encoded_test_ts = perform_autoencoder(train_ts.drop(['id'], axis=1), test_ts.drop(['id'], axis=1), encoding_dim=60, epochs=20, batch_size=32)\n",
    "encoded_train_ts, encoded_test_ts = perform_autoencoder(train_ts_filtered[stat_columns], test_ts_filtered[stat_columns], encoding_dim=20, epochs=81, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T05:22:18.799163Z",
     "iopub.status.busy": "2024-12-09T05:22:18.798833Z",
     "iopub.status.idle": "2024-12-09T05:22:18.806548Z",
     "shell.execute_reply": "2024-12-09T05:22:18.805708Z",
     "shell.execute_reply.started": "2024-12-09T05:22:18.799135Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "encoded_train_ts = pd.concat([encoded_train_ts, train_ts_filtered[non_stat_columns]], axis=1)\n",
    "\n",
    "encoded_test_ts = pd.concat([encoded_test_ts, test_ts_filtered[non_stat_columns]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T05:23:15.187374Z",
     "iopub.status.busy": "2024-12-09T05:23:15.186921Z",
     "iopub.status.idle": "2024-12-09T05:23:15.193893Z",
     "shell.execute_reply": "2024-12-09T05:23:15.192877Z",
     "shell.execute_reply.started": "2024-12-09T05:23:15.187331Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "encoded_train_ts['id'] = train_ts['id']\n",
    "encoded_test_ts['id'] = test_ts['id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process train/test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process outliers in train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T05:23:30.180282Z",
     "iopub.status.busy": "2024-12-09T05:23:30.179945Z",
     "iopub.status.idle": "2024-12-09T05:23:30.201521Z",
     "shell.execute_reply": "2024-12-09T05:23:30.200524Z",
     "shell.execute_reply.started": "2024-12-09T05:23:30.180252Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for col in train.select_dtypes(include=[np.number]).columns:  \n",
    "    negative_count = (train[col] < 0).sum() \n",
    "    if negative_count > 0:\n",
    "        print(f\"Column name: {col}, Number of negative numbers: {negative_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T05:23:31.370849Z",
     "iopub.status.busy": "2024-12-09T05:23:31.370306Z",
     "iopub.status.idle": "2024-12-09T05:23:31.828808Z",
     "shell.execute_reply": "2024-12-09T05:23:31.827895Z",
     "shell.execute_reply.started": "2024-12-09T05:23:31.370814Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def plot_boxplots(df, columns):\n",
    "    fig, axes = plt.subplots(1, len(columns), figsize=(15, 5))\n",
    "\n",
    "    for i, col in enumerate(columns):\n",
    "        axes[i].boxplot(df[col].dropna())\n",
    "        axes[i].set_title(col)\n",
    "        axes[i].set_ylabel('Value')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_boxplots(train, ['CGAS-CGAS_Score', 'BIA-BIA_BMR', 'BIA-BIA_TBW'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns ‘CGAS-CGAS_Score’, ‘BIA-BIA_BMR’, ‘BIA-BIA_TBW’ are used as examples only, outliers exist in other columns as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T05:23:49.493929Z",
     "iopub.status.busy": "2024-12-09T05:23:49.493303Z",
     "iopub.status.idle": "2024-12-09T05:23:49.499864Z",
     "shell.execute_reply": "2024-12-09T05:23:49.498949Z",
     "shell.execute_reply.started": "2024-12-09T05:23:49.493894Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "processCols = ['id','Basic_Demos-Enroll_Season', 'Basic_Demos-Age', 'Basic_Demos-Sex',\n",
    "                'CGAS-Season', 'CGAS-CGAS_Score', 'Physical-Season', 'Physical-BMI',\n",
    "                'Physical-Height', 'Physical-Weight', 'Physical-Waist_Circumference',\n",
    "                'Physical-Diastolic_BP', 'Physical-HeartRate', 'Physical-Systolic_BP',\n",
    "                'Fitness_Endurance-Season', 'Fitness_Endurance-Max_Stage',\n",
    "                'Fitness_Endurance-Time_Mins', 'Fitness_Endurance-Time_Sec',\n",
    "                'FGC-Season', 'FGC-FGC_CU', 'FGC-FGC_CU_Zone', 'FGC-FGC_GSND',\n",
    "                'FGC-FGC_GSND_Zone', 'FGC-FGC_GSD', 'FGC-FGC_GSD_Zone', 'FGC-FGC_PU',\n",
    "                'FGC-FGC_PU_Zone', 'FGC-FGC_SRL', 'FGC-FGC_SRL_Zone', 'FGC-FGC_SRR',\n",
    "                'FGC-FGC_SRR_Zone', 'FGC-FGC_TL', 'FGC-FGC_TL_Zone', 'BIA-Season',\n",
    "                'BIA-BIA_Activity_Level_num', 'BIA-BIA_BMC', 'BIA-BIA_BMI',\n",
    "                'BIA-BIA_BMR', 'BIA-BIA_DEE', 'BIA-BIA_ECW', 'BIA-BIA_FFM',\n",
    "                'BIA-BIA_FFMI', 'BIA-BIA_FMI', 'BIA-BIA_Fat', 'BIA-BIA_Frame_num',\n",
    "                'BIA-BIA_ICW', 'BIA-BIA_LDM', 'BIA-BIA_LST', 'BIA-BIA_SMM',\n",
    "                'BIA-BIA_TBW', 'PAQ_A-Season', 'PAQ_A-PAQ_A_Total', 'PAQ_C-Season',\n",
    "                'PAQ_C-PAQ_C_Total', 'SDS-Season', 'SDS-SDS_Total_Raw',\n",
    "                'SDS-SDS_Total_T', 'PreInt_EduHx-Season',\n",
    "                'PreInt_EduHx-computerinternet_hoursday', 'sii']\n",
    "\n",
    "cat_int_col = [\n",
    "    'Basic_Demos-Sex','FGC-FGC_CU_Zone','FGC-FGC_GSND_Zone','FGC-FGC_GSD_Zone','FGC-FGC_PU_Zone','FGC-FGC_SRL_Zone',\n",
    "    'FGC-FGC_SRR_Zone','FGC-FGC_TL_Zone','BIA-BIA_Activity_Level_num','BIA-BIA_Frame_num','PreInt_EduHx-computerinternet_hoursday'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T05:24:18.538147Z",
     "iopub.status.busy": "2024-12-09T05:24:18.537799Z",
     "iopub.status.idle": "2024-12-09T05:24:18.633382Z",
     "shell.execute_reply": "2024-12-09T05:24:18.632742Z",
     "shell.execute_reply.started": "2024-12-09T05:24:18.538115Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def process_outliers(train_df, test_df, multiplier=1.5):\n",
    "    # Processing negative numbers\n",
    "    y = train_df['sii']\n",
    "    train_df = train_df.drop(['sii'],axis=1)\n",
    "    numeric_cols = train_df.select_dtypes(include=['float64', 'int64']).columns\n",
    "    # Select which columns are not categorical to process outliers\n",
    "    numeric_cols = [col for col in numeric_cols if col not in cat_int_col]\n",
    "    \n",
    "    train_cleaned = train_df[numeric_cols].copy()\n",
    "    train_cleaned[train_cleaned < 0] = np.nan\n",
    "\n",
    "    bounds = {}\n",
    "\n",
    "    for col in numeric_cols:\n",
    "        # calculate Q1, Q3, IQR\n",
    "        Q1 = train_cleaned[col].quantile(0.25)\n",
    "        Q3 = train_cleaned[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "\n",
    "        # calculate upper and lower bound\n",
    "        lower_bound = Q1 - multiplier * IQR\n",
    "        upper_bound = Q3 + multiplier * IQR\n",
    "\n",
    "        \n",
    "        bounds[col] = (lower_bound, upper_bound)\n",
    "\n",
    "        # replace outliers with NaN\n",
    "        train_cleaned.loc[(train_cleaned[col] < lower_bound) | (train_cleaned[col] > upper_bound), col] = np.nan\n",
    "    \n",
    "    # process test\n",
    "    test_cleaned = test_df.copy()\n",
    "\n",
    "    for col in numeric_cols:\n",
    "        if col in bounds:\n",
    "            lower_bound, upper_bound = bounds[col]\n",
    "            test_cleaned.loc[(test_cleaned[col] < lower_bound) | (test_cleaned[col] > upper_bound), col] = np.nan\n",
    "\n",
    "    \n",
    "    for col in train_df.columns:\n",
    "        if col not in numeric_cols:\n",
    "            test_cleaned[col] = test_df[col]\n",
    "            train_cleaned[col] = train_df[col]\n",
    "            \n",
    "    train_cleaned['sii'] = y\n",
    "    return train_cleaned, test_cleaned\n",
    "\n",
    "\n",
    "train, test = process_outliers(train[processCols], test, multiplier=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set a large multiplier(15), to process the  extreme outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T05:24:22.378601Z",
     "iopub.status.busy": "2024-12-09T05:24:22.378033Z",
     "iopub.status.idle": "2024-12-09T05:24:22.752557Z",
     "shell.execute_reply": "2024-12-09T05:24:22.751667Z",
     "shell.execute_reply.started": "2024-12-09T05:24:22.378567Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "plot_boxplots(train, ['CGAS-CGAS_Score', 'BIA-BIA_BMR', 'BIA-BIA_TBW'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill the missing values in train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN and MICE (No imputer on the value of sii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T05:24:35.360517Z",
     "iopub.status.busy": "2024-12-09T05:24:35.360153Z",
     "iopub.status.idle": "2024-12-09T05:24:59.06014Z",
     "shell.execute_reply": "2024-12-09T05:24:59.058454Z",
     "shell.execute_reply.started": "2024-12-09T05:24:35.360484Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "numeric_cols = train.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "# KNN\n",
    "imputer_knn = KNNImputer(n_neighbors=5)\n",
    "imputed_data_knn = imputer_knn.fit_transform(train[numeric_cols])\n",
    "train_imputed_knn = pd.DataFrame(imputed_data_knn, columns=numeric_cols)\n",
    "train_imputed_knn['sii'] = train['sii']\n",
    "\n",
    "# MICE\n",
    "imputer_mice = IterativeImputer(min_value=0)  \n",
    "imputed_data_mice = imputer_mice.fit_transform(train[numeric_cols])\n",
    "train_imputed_mice = pd.DataFrame(imputed_data_mice, columns=numeric_cols)\n",
    "train_imputed_mice['sii'] = train['sii']\n",
    "\n",
    "int_col = cat_int_col.copy()\n",
    "int_col.append('sii')\n",
    "int_col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taking the mean for numerical data and the mode for categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T05:25:03.37584Z",
     "iopub.status.busy": "2024-12-09T05:25:03.37547Z",
     "iopub.status.idle": "2024-12-09T05:25:03.382737Z",
     "shell.execute_reply": "2024-12-09T05:25:03.381768Z",
     "shell.execute_reply.started": "2024-12-09T05:25:03.375807Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def combine_imputation_results(train_imputed_knn, train_imputed_mice, int_cols, train):\n",
    "\n",
    "    dfs = [train_imputed_knn, train_imputed_mice]\n",
    "    data_arrays = [df.values for df in dfs]\n",
    "    train_imputed_array = np.empty_like(data_arrays[0])\n",
    "\n",
    "    for j in range(train_imputed_mice.shape[1]):\n",
    "        col = train_imputed_mice.columns[j]\n",
    "        if col in int_cols:\n",
    "            values = [data_array[:, j] for data_array in data_arrays]\n",
    "            values = np.stack(values, axis=1)\n",
    "            mode = pd.DataFrame(values).mode(axis=1)[0].values\n",
    "            train_imputed_array[:, j] = mode\n",
    "        else:\n",
    "            values = [data_array[:, j] for data_array in data_arrays]\n",
    "            mean = np.mean(values, axis=0)\n",
    "            train_imputed_array[:, j] = mean\n",
    "\n",
    "    train_imputed = pd.DataFrame(train_imputed_array, \n",
    "                                 index=train_imputed_mice.index, \n",
    "                                 columns=train_imputed_mice.columns)\n",
    "\n",
    "    \n",
    "    for col in train.columns:  \n",
    "        if col not in train_imputed.columns:\n",
    "            train_imputed[col] = train[col] \n",
    "\n",
    "    return train_imputed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T05:25:09.033323Z",
     "iopub.status.busy": "2024-12-09T05:25:09.032502Z",
     "iopub.status.idle": "2024-12-09T05:25:17.944011Z",
     "shell.execute_reply": "2024-12-09T05:25:17.941771Z",
     "shell.execute_reply.started": "2024-12-09T05:25:09.033287Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_imputed = combine_imputation_results(train_imputed_knn, train_imputed_mice, int_col,train)\n",
    "train_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T05:25:22.185911Z",
     "iopub.status.busy": "2024-12-09T05:25:22.185075Z",
     "iopub.status.idle": "2024-12-09T05:25:22.194181Z",
     "shell.execute_reply": "2024-12-09T05:25:22.193218Z",
     "shell.execute_reply.started": "2024-12-09T05:25:22.185873Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train['sii'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T05:25:23.380242Z",
     "iopub.status.busy": "2024-12-09T05:25:23.379392Z",
     "iopub.status.idle": "2024-12-09T05:25:23.389013Z",
     "shell.execute_reply": "2024-12-09T05:25:23.387987Z",
     "shell.execute_reply.started": "2024-12-09T05:25:23.380193Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_imputed['sii'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T05:25:27.052848Z",
     "iopub.status.busy": "2024-12-09T05:25:27.051963Z",
     "iopub.status.idle": "2024-12-09T05:25:27.058776Z",
     "shell.execute_reply": "2024-12-09T05:25:27.057934Z",
     "shell.execute_reply.started": "2024-12-09T05:25:27.052798Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fill the missing values in test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T05:25:29.106637Z",
     "iopub.status.busy": "2024-12-09T05:25:29.106285Z",
     "iopub.status.idle": "2024-12-09T05:25:29.24928Z",
     "shell.execute_reply": "2024-12-09T05:25:29.248146Z",
     "shell.execute_reply.started": "2024-12-09T05:25:29.106603Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test['sii'] = 0\n",
    "test_imputed_knn = imputer_knn.transform(test[numeric_cols])\n",
    "test_imputed_knn = pd.DataFrame(test_imputed_knn, columns=numeric_cols)\n",
    "test_imputed_knn['sii'] = test_imputed_knn['sii'].round().astype(int)\n",
    "\n",
    "test_imputed_mice = imputer_mice.transform(test[numeric_cols])\n",
    "test_imputed_mice = pd.DataFrame(test_imputed_mice, columns=numeric_cols)\n",
    "test_imputed_mice['sii'] = test_imputed_mice['sii'].round().astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T05:25:30.446873Z",
     "iopub.status.busy": "2024-12-09T05:25:30.446038Z",
     "iopub.status.idle": "2024-12-09T05:25:30.501941Z",
     "shell.execute_reply": "2024-12-09T05:25:30.501143Z",
     "shell.execute_reply.started": "2024-12-09T05:25:30.446838Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_imputed = combine_imputation_results(test_imputed_knn, test_imputed_mice, int_col,test)\n",
    "print(test_imputed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T05:25:31.417072Z",
     "iopub.status.busy": "2024-12-09T05:25:31.416663Z",
     "iopub.status.idle": "2024-12-09T05:25:31.47203Z",
     "shell.execute_reply": "2024-12-09T05:25:31.471273Z",
     "shell.execute_reply.started": "2024-12-09T05:25:31.417043Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_imputed = test_imputed.drop(['sii'],axis=1)\n",
    "test_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T05:25:34.949933Z",
     "iopub.status.busy": "2024-12-09T05:25:34.949088Z",
     "iopub.status.idle": "2024-12-09T05:25:34.957775Z",
     "shell.execute_reply": "2024-12-09T05:25:34.956655Z",
     "shell.execute_reply.started": "2024-12-09T05:25:34.949897Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_imputed = train_imputed.dropna(subset=['sii'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T05:25:39.763377Z",
     "iopub.status.busy": "2024-12-09T05:25:39.762558Z",
     "iopub.status.idle": "2024-12-09T05:25:39.768377Z",
     "shell.execute_reply": "2024-12-09T05:25:39.767509Z",
     "shell.execute_reply.started": "2024-12-09T05:25:39.763322Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(train_imputed.shape)\n",
    "print('==========')\n",
    "print(test_imputed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering on train/test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification of blood pressure\n",
    "\n",
    "according to the AAP 2017 pediatric hypertension definitions\n",
    "\n",
    "1. https://academic.oup.com/view-large/441516841\n",
    "\n",
    "2. https://publications.aap.org/pediatrics/article/140/3/e20171904/38358/Clinical-Practice-Guideline-for-Screening-and"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T05:25:49.69676Z",
     "iopub.status.busy": "2024-12-09T05:25:49.696391Z",
     "iopub.status.idle": "2024-12-09T05:25:49.709281Z",
     "shell.execute_reply": "2024-12-09T05:25:49.708027Z",
     "shell.execute_reply.started": "2024-12-09T05:25:49.696727Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def classify_bp(train, test=None):\n",
    "    # calculate the bp percentile in train\n",
    "    sbp_percentiles = np.percentile(train.loc[train['Basic_Demos-Age'] < 13, 'Physical-Systolic_BP'], [90, 95])\n",
    "    dbp_percentiles = np.percentile(train.loc[train['Basic_Demos-Age'] < 13, 'Physical-Diastolic_BP'], [90, 95])\n",
    "    \n",
    "    # classify function\n",
    "    def classify_single_df(df, sbp_percentiles, dbp_percentiles):\n",
    "        bp_categories = []\n",
    "\n",
    "        for i in range(len(df)):\n",
    "            age = df['Basic_Demos-Age'].iloc[i]\n",
    "            sbp = df['Physical-Systolic_BP'].iloc[i]\n",
    "            dbp = df['Physical-Diastolic_BP'].iloc[i]\n",
    "\n",
    "            if age < 13:\n",
    "                if sbp < sbp_percentiles[0]:\n",
    "                    category = \"Normal\"\n",
    "                elif sbp < sbp_percentiles[1] or (sbp >= 120 and sbp < sbp_percentiles[1] and dbp < 80):\n",
    "                    category = \"Elevated\"\n",
    "                elif sbp < sbp_percentiles[1] + 12 or (sbp >= 130 and sbp < 140 and dbp >= 80 and dbp < 90):\n",
    "                    category = \"Stage 1\"\n",
    "                elif sbp >= sbp_percentiles[1] + 12 or (sbp >= 140 or dbp >= 90):\n",
    "                    category = \"Stage 2\"\n",
    "                elif sbp > sbp_percentiles[1] + 30:\n",
    "                    category = \"Hypertensive urgency\"\n",
    "                else:\n",
    "                    category = np.nan\n",
    "            else:\n",
    "                if sbp < 120 and dbp < 80:\n",
    "                    category = \"Normal\"\n",
    "                elif sbp < 130 or dbp < 80:\n",
    "                    category = \"Elevated\"\n",
    "                elif sbp < 140 or dbp < 90:\n",
    "                    category = \"Stage 1\"\n",
    "                elif sbp >= 140 or dbp >= 90:\n",
    "                    category = \"Stage 2\"\n",
    "                elif sbp > 180 or dbp > 120:\n",
    "                    category = \"Hypertensive urgency\"\n",
    "                else:\n",
    "                    category = np.nan\n",
    "\n",
    "            bp_categories.append(category)\n",
    "\n",
    "        df['BP_Category'] = bp_categories\n",
    "        return df\n",
    "    \n",
    "    # classify train\n",
    "    train = classify_single_df(train, sbp_percentiles, dbp_percentiles)\n",
    "\n",
    "    # if test exist, classify test\n",
    "    if test is not None:\n",
    "        test = classify_single_df(test, sbp_percentiles, dbp_percentiles)\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T05:25:51.46749Z",
     "iopub.status.busy": "2024-12-09T05:25:51.466853Z",
     "iopub.status.idle": "2024-12-09T05:25:51.53338Z",
     "shell.execute_reply": "2024-12-09T05:25:51.532682Z",
     "shell.execute_reply.started": "2024-12-09T05:25:51.467456Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_imputed,test_imputed = classify_bp(train_imputed,test=test_imputed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T05:25:54.234595Z",
     "iopub.status.busy": "2024-12-09T05:25:54.234259Z",
     "iopub.status.idle": "2024-12-09T05:25:54.24194Z",
     "shell.execute_reply": "2024-12-09T05:25:54.240975Z",
     "shell.execute_reply.started": "2024-12-09T05:25:54.234563Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(train_imputed['BP_Category'].value_counts())\n",
    "\n",
    "print(\"==========\")\n",
    "\n",
    "print(test_imputed['BP_Category'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FeatureEngineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T05:26:48.275208Z",
     "iopub.status.busy": "2024-12-09T05:26:48.274878Z",
     "iopub.status.idle": "2024-12-09T05:26:48.284733Z",
     "shell.execute_reply": "2024-12-09T05:26:48.283663Z",
     "shell.execute_reply.started": "2024-12-09T05:26:48.275181Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def FeatureEngineering(df):\n",
    "    season_cols = [col for col in df.columns if 'Season' in col]\n",
    "    df = df.drop(season_cols, axis=1) \n",
    "    old_list = df.columns\n",
    "    \n",
    "    df['Waist-to-Height_Ratio'] = df['Physical-Waist_Circumference'] / df['Physical-Height']\n",
    "    df['Fat_Mass'] = df['Physical-Weight'] * df['BIA-BIA_Fat']\n",
    "    df['Muscle_Fat_Ratio'] = df['BIA-BIA_SMM'] / df['BIA-BIA_FMI']\n",
    "    df['Lean_Body_Mass'] = df['Physical-Weight'] - df['Fat_Mass']\n",
    "    df['BP-HR_Ratio'] = (df['Physical-Systolic_BP'] + df['Physical-Diastolic_BP']) / df['Physical-HeartRate']\n",
    "    df['Pulse_Pressure'] = df['Physical-Systolic_BP'] - df['Physical-Diastolic_BP']\n",
    "    df['Mean_Arterial_Pressure'] = (2 * df['Physical-Diastolic_BP'] + df['Physical-Systolic_BP']) / 3\n",
    "    df['GS_diff'] = df['FGC-FGC_GSD'] - df['FGC-FGC_GSND']\n",
    "    df['GS-SMM_Ratio'] = (df['FGC-FGC_GSD'] + df['FGC-FGC_GSND'])/2 / df['BIA-BIA_SMM']\n",
    "    df['Upper_Limb_Strength'] = df['FGC-FGC_PU'] * (df['FGC-FGC_GSD'] + df['FGC-FGC_GSND']) * df['FGC-FGC_TL']\n",
    "    df['Core_Strength'] = df['FGC-FGC_CU'] * df['FGC-FGC_PU'] * df['FGC-FGC_TL']\n",
    "    df['Daily_Active'] = df['BIA-BIA_DEE'] / df['BIA-BIA_BMR']\n",
    "    df['ECW_ICW'] = df['BIA-BIA_ECW'] / df['BIA-BIA_ICW']\n",
    "    df['Endurance'] = df['Fitness_Endurance-Max_Stage'] * df['FGC-FGC_CU'] * df['FGC-FGC_PU']\n",
    "    df['Fitness_Endurance-Total_Time(sec)'] = df['Fitness_Endurance-Time_Mins'] * 60 + df['Fitness_Endurance-Time_Sec']\n",
    "    df['Sleep-Internet'] = df['SDS-SDS_Total_T'] * df['PreInt_EduHx-computerinternet_hoursday']\n",
    "    df['Sleep_Active'] = df['SDS-SDS_Total_T'] / df['PAQ_C-PAQ_C_Total']\n",
    "    df['FitnessGram_Add'] = df['FGC-FGC_CU_Zone'] + df['FGC-FGC_PU_Zone'] + df['FGC-FGC_SRL_Zone'] + df['FGC-FGC_SRR_Zone'] + df['FGC-FGC_TL_Zone']\n",
    "    \n",
    "    new_list = df.columns\n",
    "    add_list = list(set(new_list) - set(old_list))\n",
    "    return df,add_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T05:26:49.365071Z",
     "iopub.status.busy": "2024-12-09T05:26:49.364726Z",
     "iopub.status.idle": "2024-12-09T05:26:49.389943Z",
     "shell.execute_reply": "2024-12-09T05:26:49.389022Z",
     "shell.execute_reply.started": "2024-12-09T05:26:49.365041Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_imputed, FE_cols = FeatureEngineering(train_imputed)\n",
    "test_imputed, _ = FeatureEngineering(test_imputed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T05:26:50.638876Z",
     "iopub.status.busy": "2024-12-09T05:26:50.638512Z",
     "iopub.status.idle": "2024-12-09T05:26:50.6542Z",
     "shell.execute_reply": "2024-12-09T05:26:50.653083Z",
     "shell.execute_reply.started": "2024-12-09T05:26:50.638845Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_imputed.sum().tail(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T05:26:53.312382Z",
     "iopub.status.busy": "2024-12-09T05:26:53.312037Z",
     "iopub.status.idle": "2024-12-09T05:26:53.319233Z",
     "shell.execute_reply": "2024-12-09T05:26:53.318333Z",
     "shell.execute_reply.started": "2024-12-09T05:26:53.312351Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(train_imputed.shape)\n",
    "print('==========')\n",
    "print(test_imputed.shape)\n",
    "print('==========')\n",
    "print(FE_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T05:26:54.145642Z",
     "iopub.status.busy": "2024-12-09T05:26:54.145038Z",
     "iopub.status.idle": "2024-12-09T05:26:54.150394Z",
     "shell.execute_reply": "2024-12-09T05:26:54.149453Z",
     "shell.execute_reply.started": "2024-12-09T05:26:54.145609Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(encoded_train_ts.shape)\n",
    "print('==========')\n",
    "print(encoded_test_ts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T05:26:56.820032Z",
     "iopub.status.busy": "2024-12-09T05:26:56.819333Z",
     "iopub.status.idle": "2024-12-09T05:26:56.83991Z",
     "shell.execute_reply": "2024-12-09T05:26:56.839004Z",
     "shell.execute_reply.started": "2024-12-09T05:26:56.819996Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train = pd.merge(train_imputed, encoded_train_ts, how=\"outer\", on='id')\n",
    "test = pd.merge(test_imputed, encoded_test_ts, how=\"outer\", on='id')\n",
    "time_series_cols = encoded_train_ts.columns.tolist()\n",
    "time_series_cols.remove(\"id\")\n",
    "\n",
    "train = train.drop('id', axis=1)\n",
    "test = test.drop('id', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T05:26:57.895034Z",
     "iopub.status.busy": "2024-12-09T05:26:57.894331Z",
     "iopub.status.idle": "2024-12-09T05:26:57.899457Z",
     "shell.execute_reply": "2024-12-09T05:26:57.898587Z",
     "shell.execute_reply.started": "2024-12-09T05:26:57.894997Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(train.shape)\n",
    "print('==========')\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T05:26:59.088877Z",
     "iopub.status.busy": "2024-12-09T05:26:59.088205Z",
     "iopub.status.idle": "2024-12-09T05:26:59.096773Z",
     "shell.execute_reply": "2024-12-09T05:26:59.095761Z",
     "shell.execute_reply.started": "2024-12-09T05:26:59.088841Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "featuresCols = ['Basic_Demos-Age', 'Basic_Demos-Sex',\n",
    "                'CGAS-CGAS_Score', 'Physical-BMI',\n",
    "                'Physical-Height', 'Physical-Weight', 'Physical-Waist_Circumference',\n",
    "                'Physical-Diastolic_BP', 'Physical-HeartRate', 'Physical-Systolic_BP',\n",
    "                'Fitness_Endurance-Max_Stage',\n",
    "                'Fitness_Endurance-Time_Mins', 'Fitness_Endurance-Time_Sec',\n",
    "                'FGC-FGC_CU', 'FGC-FGC_CU_Zone', 'FGC-FGC_GSND',\n",
    "                'FGC-FGC_GSND_Zone', 'FGC-FGC_GSD', 'FGC-FGC_GSD_Zone', 'FGC-FGC_PU',\n",
    "                'FGC-FGC_PU_Zone', 'FGC-FGC_SRL', 'FGC-FGC_SRL_Zone', 'FGC-FGC_SRR',\n",
    "                'FGC-FGC_SRR_Zone', 'FGC-FGC_TL', 'FGC-FGC_TL_Zone', \n",
    "                'BIA-BIA_Activity_Level_num', 'BIA-BIA_BMC', 'BIA-BIA_BMI',\n",
    "                'BIA-BIA_BMR', 'BIA-BIA_DEE', 'BIA-BIA_ECW', 'BIA-BIA_FFM',\n",
    "                'BIA-BIA_FFMI', 'BIA-BIA_FMI', 'BIA-BIA_Fat', 'BIA-BIA_Frame_num',\n",
    "                'BIA-BIA_ICW', 'BIA-BIA_LDM', 'BIA-BIA_LST', 'BIA-BIA_SMM',\n",
    "                'BIA-BIA_TBW', 'PAQ_A-PAQ_A_Total',\n",
    "                'PAQ_C-PAQ_C_Total', 'SDS-SDS_Total_Raw',\n",
    "                'SDS-SDS_Total_T',\n",
    "                'PreInt_EduHx-computerinternet_hoursday', 'sii','BP_Category'\n",
    "               ]\n",
    "\n",
    "featuresCols += FE_cols\n",
    "featuresCols += time_series_cols\n",
    "\n",
    "train = train[featuresCols]\n",
    "y = train['sii']\n",
    "cat_cols=['BP_Category']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LabelEncode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T05:27:01.118367Z",
     "iopub.status.busy": "2024-12-09T05:27:01.117685Z",
     "iopub.status.idle": "2024-12-09T05:27:01.441389Z",
     "shell.execute_reply": "2024-12-09T05:27:01.440509Z",
     "shell.execute_reply.started": "2024-12-09T05:27:01.118334Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import category_encoders as ce\n",
    "\n",
    "def preprocess_cat_data(df_train, cat_cols,df_test=None):\n",
    "    for col in cat_cols:\n",
    "        df_train[col] = df_train[col].fillna('MissingValue')\n",
    "        df_test[col] = df_test[col].fillna('MissingValue')\n",
    "    \n",
    "    enc = ce.OrdinalEncoder(cols=cat_cols)  \n",
    "        \n",
    "    df_train = enc.fit_transform(df_train)\n",
    "\n",
    "    df_test = enc.transform(df_test)\n",
    "    \n",
    "    return df_train,df_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T05:27:02.686893Z",
     "iopub.status.busy": "2024-12-09T05:27:02.685753Z",
     "iopub.status.idle": "2024-12-09T05:27:02.706412Z",
     "shell.execute_reply": "2024-12-09T05:27:02.70576Z",
     "shell.execute_reply.started": "2024-12-09T05:27:02.686858Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train,test = preprocess_cat_data(train.drop(['sii'],axis=1), cat_cols, df_test=test)\n",
    "train['sii'] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T05:27:04.119892Z",
     "iopub.status.busy": "2024-12-09T05:27:04.119215Z",
     "iopub.status.idle": "2024-12-09T05:27:04.124432Z",
     "shell.execute_reply": "2024-12-09T05:27:04.12342Z",
     "shell.execute_reply.started": "2024-12-09T05:27:04.119856Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T05:27:05.622807Z",
     "iopub.status.busy": "2024-12-09T05:27:05.621911Z",
     "iopub.status.idle": "2024-12-09T05:27:05.634191Z",
     "shell.execute_reply": "2024-12-09T05:27:05.633256Z",
     "shell.execute_reply.started": "2024-12-09T05:27:05.622764Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def quadratic_weighted_kappa(y_true, y_pred):\n",
    "    return cohen_kappa_score(y_true, y_pred, weights='quadratic')\n",
    "\n",
    "def threshold_Rounder(oof_non_rounded, thresholds):\n",
    "    return np.where(oof_non_rounded < thresholds[0], 0,\n",
    "                    np.where(oof_non_rounded < thresholds[1], 1,\n",
    "                             np.where(oof_non_rounded < thresholds[2], 2, 3)))\n",
    "\n",
    "def evaluate_predictions(thresholds, y_true, oof_non_rounded):\n",
    "    rounded_p = threshold_Rounder(oof_non_rounded, thresholds)\n",
    "    return -quadratic_weighted_kappa(y_true, rounded_p)\n",
    "\n",
    "def TrainML(model_class, test_data):\n",
    "    X = train.drop(['sii'], axis=1)\n",
    "    y = train['sii']\n",
    "\n",
    "    SKF = RepeatedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=SEED)\n",
    "    \n",
    "    train_S = []\n",
    "    test_S = []\n",
    "    \n",
    "    oof_non_rounded = np.zeros(len(y), dtype=float) \n",
    "    oof_rounded = np.zeros(len(y), dtype=int) \n",
    "    test_preds = np.zeros((len(test_data), n_splits * n_repeats))\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(tqdm(SKF.split(X, y), desc=\"Training Folds\", total=n_splits)):\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "        model = clone(model_class)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        y_val_pred = model.predict(X_val)\n",
    "\n",
    "        oof_non_rounded[test_idx] = y_val_pred\n",
    "        y_val_pred_rounded = y_val_pred.round(0).astype(int)\n",
    "        oof_rounded[test_idx] = y_val_pred_rounded\n",
    "\n",
    "        train_kappa = quadratic_weighted_kappa(y_train, y_train_pred.round(0).astype(int))\n",
    "        val_kappa = quadratic_weighted_kappa(y_val, y_val_pred_rounded)\n",
    "\n",
    "        train_S.append(train_kappa)\n",
    "        test_S.append(val_kappa)\n",
    "        \n",
    "        test_preds[:, fold] = model.predict(test_data)\n",
    "        \n",
    "        print(f\"Fold {fold+1} - Train QWK: {train_kappa:.4f}, Validation QWK: {val_kappa:.4f}\")\n",
    "        clear_output(wait=True)\n",
    "\n",
    "    print(f\"Mean Train QWK --> {np.mean(train_S):.4f}\")\n",
    "    print(f\"Mean Validation QWK ---> {np.mean(test_S):.4f}\")\n",
    "\n",
    "    KappaOPtimizer = minimize(evaluate_predictions,\n",
    "                              x0=[0.5, 1.5, 2.5], args=(y, oof_non_rounded), \n",
    "                              method='Nelder-Mead')\n",
    "    assert KappaOPtimizer.success, \"Optimization did not converge.\"\n",
    "    \n",
    "    oof_tuned = threshold_Rounder(oof_non_rounded, KappaOPtimizer.x)\n",
    "    tKappa = quadratic_weighted_kappa(y, oof_tuned)\n",
    "\n",
    "    print(f\"----> || Optimized QWK SCORE :: {Fore.CYAN}{Style.BRIGHT} {tKappa:.3f}{Style.RESET_ALL}\")\n",
    "\n",
    "    tpm = test_preds.mean(axis=1)\n",
    "    tpTuned = threshold_Rounder(tpm, KappaOPtimizer.x)\n",
    "    \n",
    "    submission = pd.DataFrame({\n",
    "        'id': sample['id'],\n",
    "        'sii': tpTuned\n",
    "    })\n",
    "\n",
    "    return model, submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T05:27:07.213209Z",
     "iopub.status.busy": "2024-12-09T05:27:07.212258Z",
     "iopub.status.idle": "2024-12-09T05:27:07.216987Z",
     "shell.execute_reply": "2024-12-09T05:27:07.216084Z",
     "shell.execute_reply.started": "2024-12-09T05:27:07.213171Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import VotingRegressor, RandomForestRegressor, GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T05:27:08.349059Z",
     "iopub.status.busy": "2024-12-09T05:27:08.348741Z",
     "iopub.status.idle": "2024-12-09T05:27:08.36106Z",
     "shell.execute_reply": "2024-12-09T05:27:08.360161Z",
     "shell.execute_reply.started": "2024-12-09T05:27:08.349032Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from pytorch_tabnet.tab_model import TabNetClassifier, TabNetRegressor\n",
    "\n",
    "\n",
    "class TabNetWrapper(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.model = TabNetRegressor(**kwargs)\n",
    "        self.kwargs = kwargs\n",
    "        self.imputer = SimpleImputer(strategy='median')\n",
    "        self.best_model_path = 'best_tabnet_model.pt'\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # Handle missing values\n",
    "        X_imputed = self.imputer.fit_transform(X)\n",
    "        \n",
    "        if hasattr(y, 'values'):\n",
    "            y = y.values\n",
    "            \n",
    "        # Create internal validation set\n",
    "        X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "            X_imputed, \n",
    "            y, \n",
    "            test_size=0.2,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        # Train TabNet model\n",
    "        history = self.model.fit(\n",
    "            X_train=X_train,\n",
    "            y_train=y_train.reshape(-1, 1),\n",
    "            eval_set=[(X_valid, y_valid.reshape(-1, 1))],\n",
    "            eval_name=['valid'],\n",
    "            eval_metric=['mse'],\n",
    "            max_epochs=200,\n",
    "            patience=20,\n",
    "            batch_size=1024,\n",
    "            virtual_batch_size=128,\n",
    "            num_workers=0,\n",
    "            drop_last=False,\n",
    "            callbacks=[\n",
    "                TabNetPretrainedModelCheckpoint(\n",
    "                    filepath=self.best_model_path,\n",
    "                    monitor='valid_mse',\n",
    "                    mode='min',\n",
    "                    save_best_only=True,\n",
    "                    verbose=True\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # Load the best model\n",
    "        if os.path.exists(self.best_model_path):\n",
    "            self.model.load_model(self.best_model_path)\n",
    "            os.remove(self.best_model_path)  # Remove temporary file\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X_imputed = self.imputer.transform(X)\n",
    "        return self.model.predict(X_imputed).flatten()\n",
    "    \n",
    "    def __deepcopy__(self, memo):\n",
    "        # Add deepcopy support for scikit-learn\n",
    "        cls = self.__class__\n",
    "        result = cls.__new__(cls)\n",
    "        memo[id(self)] = result\n",
    "        for k, v in self.__dict__.items():\n",
    "            setattr(result, k, deepcopy(v, memo))\n",
    "        return result\n",
    "\n",
    "\n",
    "class TabNetPretrainedModelCheckpoint(Callback):\n",
    "    def __init__(self, filepath, monitor='val_loss', mode='min', \n",
    "                 save_best_only=True, verbose=1):\n",
    "        super().__init__()  # Initialize parent class\n",
    "        self.filepath = filepath\n",
    "        self.monitor = monitor\n",
    "        self.mode = mode\n",
    "        self.save_best_only = save_best_only\n",
    "        self.verbose = verbose\n",
    "        self.best = float('inf') if mode == 'min' else -float('inf')\n",
    "        \n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.model = self.trainer  # Use trainer itself as model\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        current = logs.get(self.monitor)\n",
    "        if current is None:\n",
    "            return\n",
    "        \n",
    "        # Check if current metric is better than best\n",
    "        if (self.mode == 'min' and current < self.best) or \\\n",
    "           (self.mode == 'max' and current > self.best):\n",
    "            if self.verbose:\n",
    "                print(f'\\nEpoch {epoch}: {self.monitor} improved from {self.best:.4f} to {current:.4f}')\n",
    "            self.best = current\n",
    "            if self.save_best_only:\n",
    "                self.model.save_model(self.filepath)  # Save the entire model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T05:27:09.341424Z",
     "iopub.status.busy": "2024-12-09T05:27:09.341062Z",
     "iopub.status.idle": "2024-12-09T05:27:09.355206Z",
     "shell.execute_reply": "2024-12-09T05:27:09.354505Z",
     "shell.execute_reply.started": "2024-12-09T05:27:09.341391Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "LGBM_Params = {\n",
    "    'learning_rate': 0.035, \n",
    "    'max_depth': 10, \n",
    "    'num_leaves': 400, \n",
    "    'min_data_in_leaf': 10,\n",
    "    'feature_fraction': 0.8, \n",
    "    'bagging_fraction': 0.75, \n",
    "    'bagging_freq': 2, \n",
    "    'lambda_l1': 5, \n",
    "    'lambda_l2': 5,\n",
    "    'device': 'gpu' if torch.cuda.is_available() else 'cpu',\n",
    "    'random_state': SEED,\n",
    "}\n",
    "\n",
    "XGB_Params = {\n",
    "    'learning_rate': 0.04,\n",
    "    'max_depth': 12,\n",
    "    'n_estimators': 200,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'reg_alpha': 1,  \n",
    "    'reg_lambda': 5,  \n",
    "    'random_state': SEED,\n",
    "    'tree_method': 'gpu_hist' if torch.cuda.is_available() else 'cpu',\n",
    "    'tree_method': 'exact'\n",
    "}\n",
    "\n",
    "HGB_Params = {\n",
    "    'learning_rate': 0.025, \n",
    "    'max_iter': 300, \n",
    "    'max_leaf_nodes': 15, \n",
    "    'min_samples_leaf': 10,\n",
    "    'l2_regularization': 5, \n",
    "    'random_state': SEED,\n",
    "}\n",
    "\n",
    "CatBoost_Params = {\n",
    "    'learning_rate': 0.025,\n",
    "    'depth': 7,\n",
    "    'iterations': 600,\n",
    "    'l2_leaf_reg': 7, \n",
    "    'random_seed': SEED,\n",
    "    'cat_features': cat_cols,\n",
    "    'subsample': 0.7,\n",
    "    'random_strength': 1.7, \n",
    "    'bagging_temperature': 0.03,\n",
    "    'border_count': 12\n",
    "}\n",
    "\n",
    "# TabNet hyperparameters\n",
    "TabNet_Params = {\n",
    "    'n_d': 64,              # Width of the decision prediction layer\n",
    "    'n_a': 64,              # Width of the attention embedding for each step\n",
    "    'n_steps': 5,           # Number of steps in the architecture\n",
    "    'gamma': 1.35,           # Coefficient for feature selection regularization\n",
    "    'n_independent': 2,     # Number of independent GLU layer in each GLU block\n",
    "    'n_shared': 2,          # Number of shared GLU layer in each GLU block\n",
    "    'lambda_sparse': 1e-5,  # Sparsity regularization\n",
    "    'optimizer_fn': torch.optim.Adam,\n",
    "    'optimizer_params': dict(lr=2e-2, weight_decay=1e-5),\n",
    "    'mask_type': 'entmax',\n",
    "    'scheduler_params': dict(mode=\"min\", patience=10, min_lr=1e-5, factor=0.5),\n",
    "    'scheduler_fn': torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
    "    'verbose': 10,\n",
    "    'device_name': 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "}\n",
    "\n",
    "LGBM_Model =LGBMRegressor(**LGBM_Params, verbose=-1,n_estimators=250)\n",
    "\n",
    "XGB_Model = XGBRegressor(**XGB_Params, verbose=-1)\n",
    "\n",
    "HGB_Model = HistGradientBoostingRegressor(**HGB_Params)\n",
    "\n",
    "CatBoost_Model = CatBoostRegressor(**CatBoost_Params)\n",
    "\n",
    "TabNet_Model = TabNetWrapper(**TabNet_Params) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T05:27:10.830423Z",
     "iopub.status.busy": "2024-12-09T05:27:10.829496Z",
     "iopub.status.idle": "2024-12-09T05:27:10.933632Z",
     "shell.execute_reply": "2024-12-09T05:27:10.932772Z",
     "shell.execute_reply.started": "2024-12-09T05:27:10.830382Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# reindex the col in test\n",
    "test = test.reindex(columns=train.columns.tolist())\n",
    "test = test.drop(['sii'],axis=1)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T05:31:10.026918Z",
     "iopub.status.busy": "2024-12-09T05:31:10.025623Z",
     "iopub.status.idle": "2024-12-09T05:40:03.806339Z",
     "shell.execute_reply": "2024-12-09T05:40:03.805515Z",
     "shell.execute_reply.started": "2024-12-09T05:31:10.026876Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Combine models using Voting Regressor\n",
    "voting_model = VotingRegressor(estimators=[\n",
    "    ('lightgbm', LGBM_Model),\n",
    "    ('xgboost', XGB_Model),\n",
    "    ('HGB', HGB_Model),\n",
    "    ('catboost', CatBoost_Model),\n",
    "    ('TabNet_Model',TabNet_Model),\n",
    "],\n",
    "   weights=[1.0,1.0,1.0,1.0,1.0]\n",
    "                              )\n",
    "\n",
    "vote_model, vote_Submission = TrainML(voting_model, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T05:40:03.80796Z",
     "iopub.status.busy": "2024-12-09T05:40:03.807629Z",
     "iopub.status.idle": "2024-12-09T05:40:03.81614Z",
     "shell.execute_reply": "2024-12-09T05:40:03.815289Z",
     "shell.execute_reply.started": "2024-12-09T05:40:03.807931Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "vote_Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-03T02:55:26.426125Z",
     "iopub.status.busy": "2024-12-03T02:55:26.425796Z",
     "iopub.status.idle": "2024-12-03T03:06:48.979674Z",
     "shell.execute_reply": "2024-12-03T03:06:48.978782Z",
     "shell.execute_reply.started": "2024-12-03T02:55:26.426099Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def extract_statistic(filename, dirname):\n",
    "    df = pd.read_parquet(os.path.join(dirname, filename, 'part-0.parquet'))\n",
    "    df.drop('step', axis=1, inplace=True)\n",
    "    return df.describe().values.reshape(-1), filename.split('=')[1]\n",
    "\n",
    "def load_ts(dirname) -> pd.DataFrame:\n",
    "    ids = os.listdir(dirname)\n",
    "    \n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        results = list(tqdm(executor.map(lambda fname: extract_statistic(fname, dirname), ids), total=len(ids)))\n",
    "    \n",
    "    stats, indexes = zip(*results)\n",
    "    \n",
    "    df = pd.DataFrame(stats, columns=[f\"stat_{i}\" for i in range(len(stats[0]))])\n",
    "    df['id'] = indexes\n",
    "    return df\n",
    "train = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/train.csv')\n",
    "test = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/test.csv')\n",
    "sample = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/sample_submission.csv')\n",
    "\n",
    "train_ts = load_ts(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_train.parquet\")\n",
    "test_ts = load_ts(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_test.parquet\")\n",
    "\n",
    "time_series_cols = train_ts.columns.tolist()\n",
    "time_series_cols.remove(\"id\")\n",
    "\n",
    "train = pd.merge(train, train_ts, how=\"left\", on='id')\n",
    "test = pd.merge(test, test_ts, how=\"left\", on='id')\n",
    "\n",
    "train = train.drop('id', axis=1)\n",
    "test = test.drop('id', axis=1)   \n",
    "\n",
    "featuresCols = ['Basic_Demos-Enroll_Season', 'Basic_Demos-Age', 'Basic_Demos-Sex',\n",
    "                'CGAS-Season', 'CGAS-CGAS_Score', 'Physical-Season', 'Physical-BMI',\n",
    "                'Physical-Height', 'Physical-Weight', 'Physical-Waist_Circumference',\n",
    "                'Physical-Diastolic_BP', 'Physical-HeartRate', 'Physical-Systolic_BP',\n",
    "                'Fitness_Endurance-Season', 'Fitness_Endurance-Max_Stage',\n",
    "                'Fitness_Endurance-Time_Mins', 'Fitness_Endurance-Time_Sec',\n",
    "                'FGC-Season', 'FGC-FGC_CU', 'FGC-FGC_CU_Zone', 'FGC-FGC_GSND',\n",
    "                'FGC-FGC_GSND_Zone', 'FGC-FGC_GSD', 'FGC-FGC_GSD_Zone', 'FGC-FGC_PU',\n",
    "                'FGC-FGC_PU_Zone', 'FGC-FGC_SRL', 'FGC-FGC_SRL_Zone', 'FGC-FGC_SRR',\n",
    "                'FGC-FGC_SRR_Zone', 'FGC-FGC_TL', 'FGC-FGC_TL_Zone', 'BIA-Season',\n",
    "                'BIA-BIA_Activity_Level_num', 'BIA-BIA_BMC', 'BIA-BIA_BMI',\n",
    "                'BIA-BIA_BMR', 'BIA-BIA_DEE', 'BIA-BIA_ECW', 'BIA-BIA_FFM',\n",
    "                'BIA-BIA_FFMI', 'BIA-BIA_FMI', 'BIA-BIA_Fat', 'BIA-BIA_Frame_num',\n",
    "                'BIA-BIA_ICW', 'BIA-BIA_LDM', 'BIA-BIA_LST', 'BIA-BIA_SMM',\n",
    "                'BIA-BIA_TBW', 'PAQ_A-Season', 'PAQ_A-PAQ_A_Total', 'PAQ_C-Season',\n",
    "                'PAQ_C-PAQ_C_Total', 'SDS-Season', 'SDS-SDS_Total_Raw',\n",
    "                'SDS-SDS_Total_T', 'PreInt_EduHx-Season',\n",
    "                'PreInt_EduHx-computerinternet_hoursday', 'sii']\n",
    "\n",
    "featuresCols += time_series_cols\n",
    "\n",
    "train = train[featuresCols]\n",
    "train = train.dropna(subset='sii')\n",
    "\n",
    "cat_c = ['Basic_Demos-Enroll_Season', 'CGAS-Season', 'Physical-Season', \n",
    "          'Fitness_Endurance-Season', 'FGC-Season', 'BIA-Season', \n",
    "          'PAQ_A-Season', 'PAQ_C-Season', 'SDS-Season', 'PreInt_EduHx-Season']\n",
    "\n",
    "def update(df):\n",
    "    global cat_c\n",
    "    for c in cat_c: \n",
    "        df[c] = df[c].fillna('Missing')\n",
    "        df[c] = df[c].astype('category')\n",
    "    return df\n",
    "        \n",
    "train = update(train)\n",
    "test = update(test)\n",
    "\n",
    "def create_mapping(column, dataset):\n",
    "    unique_values = dataset[column].unique()\n",
    "    return {value: idx for idx, value in enumerate(unique_values)}\n",
    "\n",
    "for col in cat_c:\n",
    "    mapping = create_mapping(col, train)\n",
    "    mappingTe = create_mapping(col, test)\n",
    "    \n",
    "    train[col] = train[col].replace(mapping).astype(int)\n",
    "    test[col] = test[col].replace(mappingTe).astype(int)\n",
    "\n",
    "# Combine models using Voting Regressor\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "\n",
    "voting_model = VotingRegressor(estimators=[\n",
    "    ('lgb', Pipeline(steps=[('imputer', imputer), ('regressor', LGBMRegressor(random_state=SEED))])),\n",
    "    ('xgb', Pipeline(steps=[('imputer', imputer), ('regressor', XGBRegressor(random_state=SEED))])),\n",
    "    ('cat', Pipeline(steps=[('imputer', imputer), ('regressor', CatBoostRegressor(random_state=SEED, silent=True))])),\n",
    "    ('gb', Pipeline(steps=[('imputer', imputer), ('regressor', GradientBoostingRegressor(random_state=SEED))])),\n",
    "    ('rf', Pipeline(steps=[('imputer', imputer), ('regressor', RandomForestRegressor(random_state=SEED))])),\n",
    "])\n",
    "\n",
    "# Train the ensemble model\n",
    "vote_model_1, vote_Submission_1 = TrainML(voting_model, test)\n",
    "\n",
    "# Save submission\n",
    "#Submission2.to_csv('submission.csv', index=False)\n",
    "vote_Submission_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-03T03:06:48.981516Z",
     "iopub.status.busy": "2024-12-03T03:06:48.981187Z",
     "iopub.status.idle": "2024-12-03T03:06:48.989146Z",
     "shell.execute_reply": "2024-12-03T03:06:48.988277Z",
     "shell.execute_reply.started": "2024-12-03T03:06:48.981487Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def FeatureEngineeringPlus(df):\n",
    "    season_cols = [col for col in df.columns if 'Season' in col]\n",
    "    df = df.drop(season_cols, axis=1) \n",
    "    old_list = df.columns\n",
    "    df['BMI_Age'] = df['Physical-BMI'] * df['Basic_Demos-Age']\n",
    "    df['Internet_Hours_Age'] = df['PreInt_EduHx-computerinternet_hoursday'] * df['Basic_Demos-Age']\n",
    "    df['BMI_Internet_Hours'] = df['Physical-BMI'] * df['PreInt_EduHx-computerinternet_hoursday']\n",
    "    df['BFP_BMI'] = df['BIA-BIA_Fat'] / df['BIA-BIA_BMI']\n",
    "    df['FFMI_BFP'] = df['BIA-BIA_FFMI'] / df['BIA-BIA_Fat']\n",
    "    df['FMI_BFP'] = df['BIA-BIA_FMI'] / df['BIA-BIA_Fat']\n",
    "    df['LST_TBW'] = df['BIA-BIA_LST'] / df['BIA-BIA_TBW']\n",
    "    df['BFP_BMR'] = df['BIA-BIA_Fat'] * df['BIA-BIA_BMR']\n",
    "    df['BFP_DEE'] = df['BIA-BIA_Fat'] * df['BIA-BIA_DEE']\n",
    "    df['BMR_Weight'] = df['BIA-BIA_BMR'] / df['Physical-Weight']\n",
    "    df['DEE_Weight'] = df['BIA-BIA_DEE'] / df['Physical-Weight']\n",
    "    df['SMM_Height'] = df['BIA-BIA_SMM'] / df['Physical-Height']\n",
    "    df['Muscle_to_Fat'] = df['BIA-BIA_SMM'] / df['BIA-BIA_FMI']\n",
    "    df['Hydration_Status'] = df['BIA-BIA_TBW'] / df['Physical-Weight']\n",
    "    df['ICW_TBW'] = df['BIA-BIA_ICW'] / df['BIA-BIA_TBW']\n",
    "    \n",
    "\n",
    "    new_list = df.columns\n",
    "    add_list = list(set(new_list) - set(old_list))\n",
    "    return df,add_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-03T03:06:48.99086Z",
     "iopub.status.busy": "2024-12-03T03:06:48.990457Z",
     "iopub.status.idle": "2024-12-03T03:06:49.279123Z",
     "shell.execute_reply": "2024-12-03T03:06:49.278144Z",
     "shell.execute_reply.started": "2024-12-03T03:06:48.990821Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/train.csv')\n",
    "test = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/test.csv')\n",
    "sample = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/sample_submission.csv')\n",
    "        \n",
    "# train_ts = load_ts(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_train.parquet\")\n",
    "# test_ts = load_ts(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_test.parquet\")\n",
    "train_ts = train_ts_copy\n",
    "test_ts = test_ts_copy\n",
    "\n",
    "time_series_cols = train_ts.columns.tolist()\n",
    "time_series_cols.remove(\"id\")\n",
    "\n",
    "\n",
    "train = pd.merge(train, train_ts, how=\"left\", on='id')\n",
    "test = pd.merge(test, test_ts, how=\"left\", on='id')\n",
    "\n",
    "train = train.drop('id', axis=1)\n",
    "test = test.drop('id', axis=1)   \n",
    "\n",
    "train, FE_cols = FeatureEngineeringPlus(train)\n",
    "test, _ = FeatureEngineeringPlus(test)\n",
    "\n",
    "if np.any(np.isinf(train)):\n",
    "    train = train.replace([np.inf, -np.inf], np.nan)\n",
    "if np.any(np.isinf(test)):\n",
    "    test = test.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "\n",
    "featuresCols = ['Basic_Demos-Age', 'Basic_Demos-Sex',\n",
    "                'CGAS-CGAS_Score', 'Physical-BMI',\n",
    "                'Physical-Height', 'Physical-Weight', 'Physical-Waist_Circumference',\n",
    "                'Physical-Diastolic_BP', 'Physical-HeartRate', 'Physical-Systolic_BP',\n",
    "                'Fitness_Endurance-Max_Stage',\n",
    "                'Fitness_Endurance-Time_Mins', 'Fitness_Endurance-Time_Sec',\n",
    "                'FGC-FGC_CU', 'FGC-FGC_CU_Zone', 'FGC-FGC_GSND',\n",
    "                'FGC-FGC_GSND_Zone', 'FGC-FGC_GSD', 'FGC-FGC_GSD_Zone', 'FGC-FGC_PU',\n",
    "                'FGC-FGC_PU_Zone', 'FGC-FGC_SRL', 'FGC-FGC_SRL_Zone', 'FGC-FGC_SRR',\n",
    "                'FGC-FGC_SRR_Zone', 'FGC-FGC_TL', 'FGC-FGC_TL_Zone', \n",
    "                'BIA-BIA_Activity_Level_num', 'BIA-BIA_BMC', 'BIA-BIA_BMI',\n",
    "                'BIA-BIA_BMR', 'BIA-BIA_DEE', 'BIA-BIA_ECW', 'BIA-BIA_FFM',\n",
    "                'BIA-BIA_FFMI', 'BIA-BIA_FMI', 'BIA-BIA_Fat', 'BIA-BIA_Frame_num',\n",
    "                'BIA-BIA_ICW', 'BIA-BIA_LDM', 'BIA-BIA_LST', 'BIA-BIA_SMM',\n",
    "                'BIA-BIA_TBW', 'PAQ_A-PAQ_A_Total',\n",
    "                'PAQ_C-PAQ_C_Total', 'SDS-SDS_Total_Raw',\n",
    "                'SDS-SDS_Total_T',\n",
    "                'PreInt_EduHx-computerinternet_hoursday', 'sii'\n",
    "               ]\n",
    "\n",
    "featuresCols += FE_cols\n",
    "featuresCols += time_series_cols\n",
    "\n",
    "train = train[featuresCols]\n",
    "train = train.dropna(subset='sii')\n",
    "\n",
    "cat_c = []\n",
    "\n",
    "\n",
    "def update(df):\n",
    "    global cat_c\n",
    "    for c in cat_c: \n",
    "        df[c] = df[c].fillna('Missing')\n",
    "        df[c] = df[c].astype('category')\n",
    "    return df\n",
    "        \n",
    "train = update(train)\n",
    "test = update(test)\n",
    "\n",
    "def create_mapping(column, dataset):\n",
    "    unique_values = dataset[column].unique()\n",
    "    return {value: idx for idx, value in enumerate(unique_values)}\n",
    "\n",
    "for col in cat_c:\n",
    "    mapping = create_mapping(col, train)\n",
    "    mappingTe = create_mapping(col, test)\n",
    "    \n",
    "    train[col] = train[col].replace(mapping).astype(int)\n",
    "    test[col] = test[col].replace(mappingTe).astype(int)\n",
    "# train,test = preprocess_cat_data(train, cat_c, df_test=test)\n",
    "\n",
    "# reindex the col in test\n",
    "test = test.reindex(columns=train.columns.tolist())\n",
    "test = test.drop(['sii'],axis=1)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def quadratic_weighted_kappa(y_true, y_pred):\n",
    "    return cohen_kappa_score(y_true, y_pred, weights='quadratic')\n",
    "\n",
    "def threshold_Rounder(oof_non_rounded, thresholds):\n",
    "    return np.where(oof_non_rounded < thresholds[0], 0,\n",
    "                    np.where(oof_non_rounded < thresholds[1], 1,\n",
    "                             np.where(oof_non_rounded < thresholds[2], 2, 3)))\n",
    "\n",
    "def evaluate_predictions(thresholds, y_true, oof_non_rounded):\n",
    "    rounded_p = threshold_Rounder(oof_non_rounded, thresholds)\n",
    "    return -quadratic_weighted_kappa(y_true, rounded_p)\n",
    "\n",
    "def SemiTrain(model_class, test_data):\n",
    "\n",
    "    \n",
    "    X = train.drop(['sii'], axis=1)\n",
    "    y = train['sii']\n",
    "    # Dealing with missing values (NaN), replacing NaN with -1 for LabelSpreading\n",
    "    y_filled = y.fillna(-1).astype(int)\n",
    "\n",
    "    imputer = IterativeImputer(estimator=ElasticNet(), min_value=0, # TODO:different estimator\n",
    "                               max_iter=10, random_state=SEED, \n",
    "                               initial_strategy='median')\n",
    "    # imputer = SimpleImputer(strategy='median')\n",
    "    X_imputed = imputer.fit_transform(X)  # fit_transform \n",
    "    X_imputed = pd.DataFrame(X_imputed, columns=X.columns, index=X.index)\n",
    "    test_data_imputed = pd.DataFrame(imputer.transform(test_data), columns=test_data.columns, index=test_data.index)\n",
    "    \n",
    "    SKF = RepeatedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=SEED)\n",
    "\n",
    "    train_S = []\n",
    "    test_S = []\n",
    "    \n",
    "    oof_non_rounded = np.zeros(len(y), dtype=float) \n",
    "    oof_rounded = np.zeros(len(y), dtype=int) \n",
    "    test_preds = np.zeros((len(test_data), n_splits * n_repeats))\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(tqdm(SKF.split(X_imputed, y_filled), desc=\"Training Folds\", total=n_splits* n_repeats)):\n",
    "        X_train, X_val = X_imputed.iloc[train_idx], X_imputed.iloc[test_idx]\n",
    "        y_train, y_val = y_filled.iloc[train_idx], y_filled.iloc[test_idx]\n",
    "\n",
    "        model = LabelSpreading(kernel='rbf', gamma=12, alpha=0.2) # TODO: different kernel, gamma and alpha\n",
    "        model.fit(X_train, y_train)\n",
    "        pseudo_labels = model.transduction_[y_train == -1]\n",
    "        X_train_pseudo = X_train[y_train == -1]\n",
    "        y_train_pseudo = pd.Series(pseudo_labels, index=X_train_pseudo.index)\n",
    "        \n",
    "        # Merge labelled and pseudo-labelled data\n",
    "        X_train_combined = pd.concat([X_train[y_train != -1], X_train_pseudo])\n",
    "        y_train_combined = pd.concat([y_train[y_train != -1], y_train_pseudo])\n",
    "        \n",
    "        final_model = clone(model_class)\n",
    "        final_model.fit(X_train_combined, y_train_combined)                                                              \n",
    "                                                                      \n",
    "        y_train_pred = final_model.predict(X_train[y_train != -1])  # 只在有标签数据上评估训练性能\n",
    "        y_val_pred = final_model.predict(X_val)\n",
    "\n",
    "        oof_non_rounded[test_idx] = y_val_pred\n",
    "        y_val_pred_rounded = y_val_pred.round(0).astype(int)\n",
    "        oof_rounded[test_idx] = y_val_pred_rounded\n",
    "\n",
    "        train_kappa = quadratic_weighted_kappa(y_train[y_train != -1], y_train_pred.round(0).astype(int)) \n",
    "        val_kappa = quadratic_weighted_kappa(y_val[y_val != -1], y_val_pred_rounded[y_val != -1]) \n",
    "\n",
    "        train_S.append(train_kappa)\n",
    "        test_S.append(val_kappa)\n",
    "        \n",
    "        test_preds[:, fold] = final_model.predict(test_data_imputed)\n",
    "        \n",
    "        print(f\"Fold {fold+1} - Train QWK: {train_kappa:.4f}, Validation QWK: {val_kappa:.4f}\")\n",
    "        clear_output(wait=True)\n",
    "\n",
    "    print(f\"Mean Train QWK --> {np.mean(train_S):.4f}\")\n",
    "    print(f\"Mean Validation QWK ---> {np.mean(test_S):.4f}\")\n",
    "\n",
    "    KappaOPtimizer = minimize(evaluate_predictions,\n",
    "                              x0=[0.5, 1.5, 2.5], args=(y[y.notna()], oof_non_rounded[y.notna()]), \n",
    "                              method='Nelder-Mead')\n",
    "    assert KappaOPtimizer.success, \"Optimization did not converge.\"\n",
    "    \n",
    "    oof_tuned = threshold_Rounder(oof_non_rounded, KappaOPtimizer.x)\n",
    "    tKappa = quadratic_weighted_kappa(y[y.notna()], oof_tuned[y.notna()]) \n",
    "\n",
    "    print(f\"----> || Optimized QWK SCORE :: {Fore.CYAN}{Style.BRIGHT} {tKappa:.3f}{Style.RESET_ALL}\")\n",
    "\n",
    "    tpm = test_preds.mean(axis=1)\n",
    "    tpTuned = threshold_Rounder(tpm, KappaOPtimizer.x)\n",
    "    \n",
    "    submission = pd.DataFrame({\n",
    "        'id': sample['id'],\n",
    "        'sii': tpTuned\n",
    "    })\n",
    "\n",
    "    return final_model, submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-03T03:06:49.281463Z",
     "iopub.status.busy": "2024-12-03T03:06:49.281128Z",
     "iopub.status.idle": "2024-12-03T03:11:47.878769Z",
     "shell.execute_reply": "2024-12-03T03:11:47.8779Z",
     "shell.execute_reply.started": "2024-12-03T03:06:49.281424Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Model parameters for LightGBM\n",
    "LGBM_Params1 = {\n",
    "    'learning_rate': 0.045,\n",
    "    'max_depth': 7,\n",
    "    'num_leaves': 350,\n",
    "    'min_data_in_leaf': 13,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 4,\n",
    "    'lambda_l1': 10,  # Increased from 6.59\n",
    "    'lambda_l2': 0.01,  # Increased from 2.68e-06\n",
    "    'random_state': SEED\n",
    "}\n",
    "\n",
    "\n",
    "# XGBoost parameters\n",
    "XGB_Params1 = {\n",
    "    'learning_rate': 0.045,\n",
    "    'max_depth': 7,\n",
    "    'n_estimators': 200,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'reg_alpha': 1,  # Increased from 0.1\n",
    "    'reg_lambda': 5,  # Increased from 1\n",
    "    'random_state': SEED\n",
    "}\n",
    "\n",
    "\n",
    "CatBoost_Params1 = {\n",
    "    'learning_rate': 0.045,\n",
    "    'depth': 7,\n",
    "    'iterations': 200,\n",
    "    'random_seed': SEED,\n",
    "    'cat_features': cat_c,\n",
    "    'verbose': 0,\n",
    "    'l2_leaf_reg': 10  # Increase this value\n",
    "}\n",
    "from sklearn.linear_model import BayesianRidge,ElasticNet\n",
    "\n",
    "# Create model instances\n",
    "Light = LGBMRegressor(**LGBM_Params1,  verbose=-1, n_estimators=300)\n",
    "XGB_Model = XGBRegressor(**XGB_Params1)\n",
    "CatBoost_Model = CatBoostRegressor(**CatBoost_Params1)\n",
    "\n",
    "# Combine models using Voting Regressor\n",
    "\n",
    "ensemble = VotingRegressor(estimators=[\n",
    "    ('lightgbm', Light),\n",
    "    ('xgboost', XGB_Model),\n",
    "    ('catboost', CatBoost_Model)\n",
    "])\n",
    "\n",
    "vote_model_2, vote_Submission_2 = SemiTrain(ensemble, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-03T03:11:47.880073Z",
     "iopub.status.busy": "2024-12-03T03:11:47.879804Z",
     "iopub.status.idle": "2024-12-03T03:11:47.905441Z",
     "shell.execute_reply": "2024-12-03T03:11:47.904537Z",
     "shell.execute_reply.started": "2024-12-03T03:11:47.880048Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "vote_Submission = vote_Submission.sort_values(by='id').reset_index(drop=True)\n",
    "vote_Submission_1 = vote_Submission_1.sort_values(by='id').reset_index(drop=True)\n",
    "vote_Submission_2 = vote_Submission_2.sort_values(by='id').reset_index(drop=True)\n",
    "\n",
    "combined = pd.DataFrame({\n",
    "    'id': vote_Submission['id'],\n",
    "    'sii_1': vote_Submission['sii'],\n",
    "    'sii_2': vote_Submission_1['sii'],\n",
    "    'sii_3': vote_Submission_2['sii']\n",
    "})\n",
    "\n",
    "def majority_vote(row):\n",
    "    return row.mode()[0]\n",
    "\n",
    "combined['final_sii'] = combined[['sii_1', 'sii_2', 'sii_3']].apply(majority_vote, axis=1)\n",
    "\n",
    "final_submission = combined[['id', 'final_sii']].rename(columns={'final_sii': 'sii'})\n",
    "\n",
    "final_submission.to_csv('submission.csv', index=False)\n",
    "final_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-03T03:11:47.906808Z",
     "iopub.status.busy": "2024-12-03T03:11:47.906507Z",
     "iopub.status.idle": "2024-12-03T03:11:47.914099Z",
     "shell.execute_reply": "2024-12-03T03:11:47.91328Z",
     "shell.execute_reply.started": "2024-12-03T03:11:47.906777Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"finish\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 9643020,
     "sourceId": 81933,
     "sourceType": "competition"
    },
    {
     "datasetId": 921302,
     "sourceId": 7453542,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 210223252,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30762,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
